{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2044a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pandas와 openpyxl 라이브러리 다운로드 후, import 해준다.\n",
    "# import pandas as pd\n",
    "\n",
    "# # 원하는 엑셀 파일을 pd.read_excel('') 함수를 이용해 데이터프레임으로 만들어준다.\n",
    "# df1 = pd.read_excel('game_1_eng.xlsx')\n",
    "# df2 = pd.read_excel('game_2_eng.xlsx')\n",
    "\n",
    "# # merge 함수를 이용해 합쳐준다. left는 df1 기준으로 df2가 붙는다는 뜻.\n",
    "# df3 = pd.merge(df1, df2)\n",
    "\n",
    "# # 해당 데이터프레임을 엑셀로 저장해준다.\n",
    "# df3.to_excel('test_merge.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a237f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\multicampus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 57068\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(\"gamedata_eng.xlsx\")\n",
    "print('Total number of documents:', len(df))\n",
    "\n",
    "# Preview the first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Data cleaning functions\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in str(s) if  ord(i) < 128)\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# # Data cleaning\n",
    "# df['cleaned'] = df['about_the_game'].apply(_removeNonAscii)\n",
    "# df['cleaned'] = df['cleaned'].apply(make_lower_case)\n",
    "# df['cleaned'] = df['cleaned'].apply(remove_stop_words)\n",
    "# df['cleaned'] = df['cleaned'].apply(remove_punctuation)\n",
    "# df['cleaned'] = df['cleaned'].apply(remove_html)\n",
    "\n",
    "# # Remove empty rows\n",
    "# df['cleaned'].replace('', np.nan, inplace=True)\n",
    "# df = df[df['cleaned'].notna()]\n",
    "# print('Total number of documents after cleaning:', len(df))\n",
    "\n",
    "# # Build Word2Vec model\n",
    "# corpus = [words.split() for words in df['cleaned']]\n",
    "# word2vec_model = Word2Vec(size=300, window=5, min_count=2, workers=-1) # make model\n",
    "# word2vec_model.build_vocab(corpus) # \n",
    "# word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True) # pre-trained data\n",
    "# word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=15) \n",
    "\n",
    "# # Get document vectors\n",
    "# def get_document_vectors(document_list, model):\n",
    "#     document_embedding_list = []\n",
    "    \n",
    "#     for line in document_list:\n",
    "#         doc2vec = None\n",
    "#         count = 0\n",
    "#         for word in line.split():\n",
    "#             if word in model.wv.vocab:\n",
    "#                 count += 1\n",
    "#                 if doc2vec is None:\n",
    "#                     doc2vec = model[word]\n",
    "#                 else:\n",
    "#                     doc2vec = doc2vec + model[word]\n",
    "#         if doc2vec is not None:\n",
    "#             doc2vec = doc2vec / count\n",
    "#             document_embedding_list.append(doc2vec)\n",
    "    \n",
    "#     return document_embedding_list\n",
    "\n",
    "# document_embedding_list = get_document_vectors(df['cleaned'], word2vec_model)\n",
    "# print('Number of document vectors:', len(document_embedding_list))\n",
    "# print(document_embedding_list)\n",
    "\n",
    "# # Calculate cosine similarity matrix\n",
    "# cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)\n",
    "# print('Size of cosine similarity matrix:', cosine_similarities.shape)\n",
    "\n",
    "# def get_movie_indices(name, df, cosine_similarities):\n",
    "#     # Get the index of the movie based on its name\n",
    "#     indices = pd.Series(df.index, index=df['name']).drop_duplicates()\n",
    "#     idx = indices[name]\n",
    "\n",
    "#     # Get the indices of the most similar movies based on the cosine similarities\n",
    "#     sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "#     sim_scores = sim_scores[1:6]\n",
    "#     movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "#     return movie_indices\n",
    "\n",
    "# def show_recommendations(name, df, cosine_similarities):\n",
    "#     movies = df[['name', 'image']]\n",
    "\n",
    "#     # Get the indices of the recommended movies\n",
    "#     movie_indices = get_movie_indices(name, df, cosine_similarities)\n",
    "\n",
    "#     # Get the recommended movies based on their indices\n",
    "#     recommend = movies.iloc[movie_indices].reset_index(drop=True)\n",
    "\n",
    "#     fig, axs = plt.subplots(1, 5, figsize=(20, 30))\n",
    "\n",
    "#     # Display the images of the recommended movies\n",
    "#     for index, row in recommend.iterrows():\n",
    "#         try:\n",
    "#             response = requests.get(row['image'])\n",
    "#             img = Image.open(BytesIO(response.content))\n",
    "#             axs[index].imshow(img)\n",
    "#             axs[index].set_title(row['name'])\n",
    "#         except:\n",
    "#             continue\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97678cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents after cleaning: 57051\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data cleaning\n",
    "df['cleaned'] = df['about_the_game'].apply(_removeNonAscii)\n",
    "df['cleaned'] = df['cleaned'].apply(make_lower_case)\n",
    "df['cleaned'] = df['cleaned'].apply(remove_stop_words)\n",
    "df['cleaned'] = df['cleaned'].apply(remove_punctuation)\n",
    "df['cleaned'] = df['cleaned'].apply(remove_html)\n",
    "\n",
    "# Remove empty rows\n",
    "df['cleaned'].replace('', np.nan, inplace=True)\n",
    "df = df[df['cleaned'].notna()]\n",
    "print('Total number of documents after cleaning:', len(df))\n",
    "\n",
    "# Build Word2Vec model\n",
    "corpus = [words.split() for words in df['cleaned']]\n",
    "word2vec_model = Word2Vec(size=300, window=5, min_count=2, workers=-1) # make model\n",
    "word2vec_model.build_vocab(corpus) # \n",
    "word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True) # pre-trained data\n",
    "word2vec_model.train(corpus, total_examples=word2vec_model.corpus_count, epochs=15) \n",
    "\n",
    "# Get document vectors\n",
    "def get_document_vectors(document_list, model):\n",
    "    document_embedding_list = []\n",
    "    \n",
    "    for line in document_list:\n",
    "        doc2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in model.wv.vocab:\n",
    "                count += 1\n",
    "                if doc2vec is None:\n",
    "                    doc2vec = model[word]\n",
    "                else:\n",
    "                    doc2vec = doc2vec + model[word]\n",
    "        if doc2vec is not None:\n",
    "            doc2vec = doc2vec / count\n",
    "            document_embedding_list.append(doc2vec)\n",
    "    \n",
    "    return document_embedding_list\n",
    "\n",
    "document_embedding_list = get_document_vectors(df['cleaned'], word2vec_model)\n",
    "print('Number of document vectors:', len(document_embedding_list))\n",
    "print(document_embedding_list)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)\n",
    "print('Size of cosine similarity matrix:', cosine_similarities.shape)\n",
    "\n",
    "def get_movie_indices(name, df, cosine_similarities):\n",
    "    # Get the index of the movie based on its name\n",
    "    indices = pd.Series(df.index, index=df['name']).drop_duplicates()\n",
    "    idx = indices[name]\n",
    "\n",
    "    # Get the indices of the most similar movies based on the cosine similarities\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    return movie_indices\n",
    "\n",
    "def show_recommendations(name, df, cosine_similarities):\n",
    "    movies = df[['name', 'image']]\n",
    "\n",
    "    # Get the indices of the recommended movies\n",
    "    movie_indices = get_movie_indices(name, df, cosine_similarities)\n",
    "\n",
    "    # Get the recommended movies based on their indices\n",
    "    recommend = movies.iloc[movie_indices].reset_index(drop=True)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 30))\n",
    "\n",
    "    # Display the images of the recommended movies\n",
    "    for index, row in recommend.iterrows():\n",
    "        try:\n",
    "            response = requests.get(row['image'])\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            axs[index].imshow(img)\n",
    "            axs[index].set_title(row['name'])\n",
    "        except:\n",
    "            continue\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3eb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recommendations(\"ENEMIES\", df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24573911",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recommendations(\"Zombie Freaks\", df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b84479",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recommendations(\"The Hell Hives\", df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recommendations(\"MAZE: A VR Adventure\", df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recommendations(\"Woods of Death 2\", df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec307b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
